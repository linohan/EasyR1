# 启动head节点
``` bash
ray start --head --temp-dir /data_nvme3n1/tmp/ray --port=6379 --include-dashboard=true --dashboard-host=0.0.0.0 --dashboard-port=8265
```
1）启动ray时务必把`--temp-dir`参数设置成一个空间大的硬盘目录，如果使用默认的系统盘目录，运行训练任务时的checkpoints文件会占用大量的系统盘空间。
2）启动Master节点并将dashboard设置为任何ip都可以访问。
3）这里会显示Master节点的ip地址，后面的其它节点连接Master节点时需要用到。
# 其它节点连接ray集群：
``` bash
ray start --address='master node ip:6379'
```

# 查看ray的状态
``` bash
ray status
```

# 停止ray集群
``` bash
ray stop
```

# 测试ray连接代码
``` python
import ray
import time
ray.init(address="master node ip:6379")
@ray.remote
def f1():
    time.sleep(1)
    return 0
res = ray.get([ f1.remote() for _ in range(50)])
```

# 查看job
Then you can check the job status with the following commands:

ray job list: list all jobs submitted to the cluster.

ray job logs 【Submission ID】': query the logs of the job.

ray job status 【Submission ID】: query the status of the job.

ray job stop --no-wait 【Submission ID】: request the job to be stopped.

You can also access driver/task/actor logs in /tmp/ray/session_latest/logs/, driver log is job-driver-raysubmit_<Submission ID>.log.

We strongly recommend you to view job detail from dashboard in multinode training, because it provide more structure way to view the job information.
在浏览器中输入`master node ip:8265`即可查看。